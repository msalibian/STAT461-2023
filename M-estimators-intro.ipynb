{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to M-estimators\n",
    "\n",
    "#### LICENSE\n",
    "These notes are released under the \n",
    "\"Creative Commons Attribution-ShareAlike 4.0 International\" license. \n",
    "See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). \n",
    "\n",
    "#### INSTALLATION instructions\n",
    "\n",
    "To use this noteboook you may need to install a few packages in `R`:\n",
    "```\n",
    "install.packages(c('rmutil', 'robustbase', 'RobStatTM'))\n",
    "```\n",
    "\n",
    "## Intro\n",
    "\n",
    "In this notebook we will review simple location M-estimators, some of their \n",
    "robustness properties, and algorithms to compute them. \n",
    "\n",
    "We first start by loading a simple data set `robustbase::cushny`. Refer to \n",
    "`help(cushny, package='robustbase')` for information on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x <- robustbase::cushny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxplot(x, col='tomato3', cex=1.5, pch=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rbind(mean = mean(x), median = median(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute an M-estimator, using a Huber loss, and without standardizing. We \n",
    "write our own code, using the iterative weighted averages algorithm discussed in class. \n",
    "\n",
    "First we define the derivative of the loss function rho (which, in the robustness literature, is often called \"Psi\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HuberPsi <- function(r, cc)\n",
    "    return( pmin(pmax(-cc, r), cc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `mest0` below takes as arguments the data, the tuning constant `c` (the place where the loss functions transitions from a squared loss to a linear one), a initial estimator \"mu(0)\" to start the iterations, the maximum number of allowed iterations (default of `100`) and the default convergence tolerance (the algorithm will stop when two consecutive values of the estimated mu differ in less than `eps`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mest0 <- function(x, cc=1.345, init=median(x), max.it = 100, eps=1e-8) {\n",
    "    m1 <- init\n",
    "    m0 <- m1 + 10*eps\n",
    "    it <- 0\n",
    "    while( ((it <- it+1) < max.it ) & (abs(m1-m0) > eps ) ) {\n",
    "        re <- (x - m1)\n",
    "        w <- HuberPsi(re, cc=cc)/re\n",
    "        w[ is.na(w) ] <- 1\n",
    "        m0 <- m1\n",
    "        m1 <- sum( x*w ) / sum(w)\n",
    "    }\n",
    "    return(m1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function on the data in `x` to compute the M-estimator (note that we let all the other arguments take their default values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mu0 <- mest0(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the M-estimator is actually \"between\" the mean and the median. \n",
    "\n",
    "It is always a good habit to perform a quick sanity check and verify that the estimator satisfies the estimating equations (in this case, that the first order conditions are met): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean( HuberPsi(x-mu0, cc=1.345)) # this should be essentially zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lack of scale invariance, robustness\n",
    "\n",
    "As we discussed in class, this estimator is not scale equivariant. For example, if we divide all the data by 100, and then multiply the resulting estimator by 100, we do not recover the original estimator. In fact, something much more \"surprising\" happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rbind(mean=c(mean(x), mean(x/100)*100),\n",
    "      median=c(median(x), median(x/100)*100),\n",
    "      Mest=c(mest0(x), mest0(x/100)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The suppossedly robust M-estimator computed on the \"scaled\" data is identical to the sample mean! This is a serious problem, as the estimator is not robust any longer. As discussed in class, the problem is that the tuning parameter (the choice of loss function rho depends on the \"size\" of the data / residuals). \n",
    "\n",
    "We now add 2 outliers to illustrate that this non-scale-equivariant M-estimator really is not robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xc <- c(x, rnorm(2, mean=5.5, sd=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the estimators again. Note that the performance of the M-estimator deteriorates (it appears to be affected by the outliers), but not as much as the sample mean.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rbind(mean=c(mean(x), mean(xc)),\n",
    "      median=c(median(x), median(xc)),\n",
    "      Mest=c(mest0(x), mest0(xc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To again illustrate the problem of the relative magnitudes of the data and the tuning constant of the (hopefully) robust loss, we compute the estimators on \"proportionally smaller\" data, and then re-scale it back to the original units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rbind(mean=c(mean(x), mean(xc), mean(xc/100)*100),\n",
    "      median=c(median(x), median(xc), median(xc/100)*100),\n",
    "      Mest=c(mest0(x), mest0(xc), mest0(xc/100)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clearly see the deterioration of the M-estimator. It is just not working well. \n",
    "\n",
    "### Another illustration of this problem (synthetic)\n",
    "\n",
    "We can also see the problem directly on data that have a much smaller scale than the default tuning parameter we use above. The following is a simple synthetic example. Generate a sample of 25 observations from a N(0.1, 0.01) distribution (variance = 0.01), and add 3 outliers located around 1 (note that they are located 0.9 / 0.1 = 9 standard deviations away from the true mean!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "x2 <- rnorm(25, mean=0.1, sd=.1)\n",
    "x2c <- c(x2, rnorm(5, mean=1, sd=.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare the sample mean, median and the \"robust\" non-scale-equivariant M-estimator, on the clean and contaminated data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rbind(mean=c(mean(x2), mean(x2c)),\n",
    "      median=c(median(x2), median(x2c)),\n",
    "      Mest=c(mest0(x2), mest0(x2c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how for this data the M-estimator is **identical** to the **sample mean**! \n",
    "\n",
    "Can you explain why this is, in fact, to be expected here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scaled residuals helps in choosing the robust loss\n",
    "\n",
    "The solution, as we discussed in more detail in class, is to use standardized residuals. The only difference between the \"good\" M-estimator computed with `mest` below and the previous one (`mest0`) is the inclusion of the robust scale estimator (`si <- mad(x)`), and its use in the computation of residuals (`re <- (x - m1) / si`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mest <- function(x, cc=1.345, init=median(x), si = mad(x), max.it = 100, eps=1e-8) {\n",
    "    m1 <- init\n",
    "    m0 <- m1 + 10*eps\n",
    "    it <- 0\n",
    "    while( ((it <- it+1) < max.it ) & (abs(m1-m0) > eps ) ) {\n",
    "        re <- (x - m1) / si\n",
    "        w <- HuberPsi(re, cc=cc)/re\n",
    "        w[ is.na(w) ] <- 1\n",
    "        m0 <- m1\n",
    "        m1 <- sum( x*w ) / sum(w)\n",
    "    }\n",
    "    return(m1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now everything works fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rbind(mean=c(mean(x), mean(xc), mean(xc/100)*100),\n",
    "      median=c(median(x), median(xc), median(xc/100)*100),\n",
    "      Mest=c(mest(x), mest(xc), mest(xc/100)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check again. First order conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si <- mad(xc)\n",
    "mu1 <- mest(xc)\n",
    "mean( HuberPsi((xc-mu1)/si, cc=1.345))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also works fine for the artificial example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rbind(mean=c(mean(x2), mean(x2c)),\n",
    "      median=c(median(x2), median(x2c)),\n",
    "      Mest=c(mest(x2), mest(x2c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-estimators are robust, not immutable\n",
    "\n",
    "Note, however, that the M-estimator is in fact, affected by the outliers. Fortunately, this effect is bounded, and will not get any worse even if the outliers were much more extreme. For example, if the outliers were placed at `+20` (instead of `5.5`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xc2 <- c(x, rnorm(2, mean=20, sd=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then the M-estimator does not shift any further to the right, as opposed to what happens with the sample mean: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rbind(mean=c(mean(x), mean(xc), mean(xc2)),\n",
    "      median=c(median(x), median(xc), median(xc2)),\n",
    "      Mest=c(mest(x), mest(xc), mest(xc2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-estimators with a bounded loss function\n",
    "\n",
    "M-estimators with a bounded loss (re-descending score function) have a different behaviour, and usually offer better bias performance (they deviate less than Huber-type M-estimators) when outliers grow. \n",
    "\n",
    "Bounded (but not constant, and thus necessarily non-convex) loss functions will play a key role in the linear regression context. Note that a bounded loss function has a derivative (score function) that becomes zero for large residuals (instead of constant for Huber-type loss functions). \n",
    "\n",
    "A commonly used family of such loss / score functions is Tukey's bisquare family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TukeyPsi <- function(r, cc) {\n",
    "    tmp <- r/cc\n",
    "    tmp2 <- tmp*(1-tmp^2)^2\n",
    "    tmp2[ abs(tmp) > 1 ] <- 0\n",
    "    return(tmp2)\n",
    "}\n",
    "xx <- seq(-3, 3, length=1000)\n",
    "plot(xx, TukeyPsi(xx, cc=2.5), type='l', lwd=4, col='hotpink',\n",
    "    ylab = expression(Psi[c]), cex.lab=1.5, xlab='t')\n",
    "abline(h=0); abline(v=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding weight function (associated with the iterative re-weighted averages algorithm) is also qualitatively different from Huber's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TukeyW <- function(r, cc) {\n",
    "    tmp <- TukeyPsi(r, cc) / r\n",
    "    tmp[is.na(tmp)] <- (1/cc)\n",
    "    return(tmp)\n",
    "}\n",
    "xx <- seq(-3, 3, length=1000)\n",
    "plot(xx, TukeyW(xx, cc=2.5), type='l', lwd=4, col='seagreen4',\n",
    "    ylab = expression(Psi[c]), cex.lab=1.5, xlab='t')\n",
    "abline(h=0); abline(v=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What tuning constant should we use to obtain 95% efficiency if the data are Gaussian? Compute the asymptotic variance and pick `cc` so that when the data are Gaussian it is (approximately) 1.05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TukeyPsiprime <- function(r, cc) {\n",
    "    tmp <- r/cc\n",
    "    tmp2 <- (1/cc)*(1-tmp^2)*(1-5*tmp^2)\n",
    "    tmp2[ abs(tmp) > 1 ] <- 0\n",
    "    return(tmp2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 <- function(r, cc) dnorm(r)*TukeyPsi(r, cc)^2\n",
    "f2 <- function(r, cc) dnorm(r)*TukeyPsiprime(r, cc)\n",
    "a1 <- integrate(f1, lower=-Inf, upper=+Inf, cc=4.65)\n",
    "a2 <- integrate(f2, lower=-Inf, upper=+Inf, cc=4.65)\n",
    "a1$message\n",
    "a2$message\n",
    "a1$value / (a2$value^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now adapt the `mest` function to use Tukey's bisquare loss function, call it `mest2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mest2 <- function(x, cc=4.65, init=median(x), si = mad(x), max.it = 100, eps=1e-8) {\n",
    "    m1 <- init\n",
    "    m0 <- m1 + 10*eps\n",
    "    it <- 0\n",
    "    while( ((it <- it+1) < max.it ) & (abs(m1-m0) > eps ) ) {\n",
    "        re <- (x - m1) / si\n",
    "        w <- TukeyPsi(re, cc=cc)/re\n",
    "        w[ is.na(w) ] <- 1\n",
    "        m0 <- m1\n",
    "        m1 <- sum( x*w ) / sum(w)\n",
    "    }\n",
    "    return(m1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of the \"spring\" behaviour. We build a \"clean\" data set from a N(2, 1/4) distribution, and add outliers centred around 4.5 (this is the object `xc` below), and also another sample where outliers were added around 9 (`xc2`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "x <- rnorm(40, mean=2, sd=.5)\n",
    "xc <- c(x, rnorm(15, mean=5, sd=.25))\n",
    "xc2 <- c(x, rnorm(15, mean=10, sd=.25))\n",
    "boxplot(x, xc, xc2, col=c('steelblue', 'seagreen2', 'tomato3'),\n",
    "       names=c('Clean', 'Outs @ 5', 'Outs @ 10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a <- rbind(mean=c(mean(x), mean(xc), mean(xc2)),\n",
    "      median=c(median(x), median(xc), median(xc2)),\n",
    "      Mest=c(mest(x), mest(xc), mest(xc2)),\n",
    "      RedMest = c(mest2(x), mest2(xc), mest2(xc2)))\n",
    "colnames(a) <- c('Clean', 'Outs @ 5', 'Outs @ 10')\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
