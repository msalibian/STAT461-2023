{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ecb37a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"STAT461 - Lecture notes\"\n",
    "author: \"Matias Salibian-Barrera\"\n",
    "date: \"`r format(Sys.Date())`\"\n",
    "html_document: \n",
    "     theme: lumen \n",
    "---\n",
    "\n",
    "\n",
    "<!-- output: github_document -->\n",
    "<!-- --- -->\n",
    "\n",
    "<!-- notedown --nomagic Boot1.Rmd > Boot1.ipynb -->\n",
    "\n",
    "<style type=\"text/css\">\n",
    "  body{\n",
    "  font-size: 12pt;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c238e5",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "include": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE, fig.width=8, \n",
    "fig.height=8, message=FALSE, warning=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8054be3",
   "metadata": {},
   "source": [
    "#### LICENSE\n",
    "These notes are released under the \n",
    "\"Creative Commons Attribution-ShareAlike 4.0 International\" license. \n",
    "See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). \n",
    "\n",
    "# Simple examples of robust estimators for linear regression\n",
    "\n",
    "### Simple linear regression\n",
    "We will use the `phosphor` data in \n",
    "package `RobStatTM` (for `R`). \n",
    "Details can be found using `help(phosphor, package='RobStatTM')`. \n",
    "The response variable is `plant` and, \n",
    "to simplify the example, we will use only one explanatory variable,\n",
    "`organic`. Furthermore, in order to \n",
    "highlight the potential impact of outliers, we will \n",
    "change the position of the single outlier in these data (from the \n",
    "right end of the plot to the left):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(robustbase)\n",
    "data(phosphor)\n",
    "# library(RobStatTM)\n",
    "# artificially change the location of the outlier \n",
    "# for illustration purposes\n",
    "phosphor[17, 'organic'] <- 15\n",
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b1a75",
   "metadata": {},
   "source": [
    "We now fit a robust estimator for linear regression models \n",
    "(an MM-estimator) and overlay it in red on the above plot. \n",
    "The usual least squares estimator is shown in blue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMfit <- lmrobdetMM(plant ~ organic, data=phosphor)\n",
    "MMfit <- lmrob(plant ~ organic, data=phosphor)\n",
    "LSfit <- lm(plant ~ organic, data=phosphor)\n",
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')\n",
    "abline(MMfit, lwd=4, col='tomato3')\n",
    "abline(LSfit, lwd=4, col='steelblue3', lty=1)\n",
    "legend('topright', lwd=3, lty=c(1, 2), col=c('tomato3', 'steelblue3'), \n",
    "       legend=c('MM', 'LS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11923835",
   "metadata": {},
   "source": [
    "<!-- We look at the estimated regression coefficients: -->\n",
    "<!-- ```{r coef} -->\n",
    "<!-- cbind(MM=coef(MMfit), LS=coef(LSfit)) -->\n",
    "<!-- ``` -->\n",
    "Note that if outliers were not present in the data, then the \n",
    "robust and the least squares estimator coincide. \n",
    "The green line in the next plot corresponds to the OLS estimator\n",
    "computed without the outlier. Note that the robust fit \n",
    "is indistinguishable from this last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')\n",
    "abline(MMfit, lwd=3, col='tomato3')\n",
    "abline(LSfit, lwd=3, col='steelblue3')\n",
    "LSclean <- lm(plant ~ organic, data=phosphor, subset=-17)\n",
    "abline(LSclean, lwd=3, col='green3')\n",
    "legend('topright', lwd=3, lty=1, col=c('tomato3', 'steelblue3', 'green3'), \n",
    "       legend=c('MM', 'LS', 'LS(clean)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419eec60",
   "metadata": {},
   "source": [
    "We can also see that the estimated regression coefficients are\n",
    "in fact very close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe335a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbind(MM=coef(MMfit), LS=coef(LSfit), LSclean=coef(LSclean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774366cd",
   "metadata": {},
   "source": [
    "Note that the MM-estimator is indistinguishable from the \n",
    "LS estimator computed on the clean data. This is the desired\n",
    "result of using an efficient and robust estimator. \n",
    "\n",
    "\n",
    "#### A synthetic toy example (diagnostics and estimation)\n",
    "\n",
    "This example will illustrate that:\n",
    "\n",
    "- outliers can be severely damaging without being \"obviously\" apparent;\n",
    "- quantile regression estimators (L1) offer limited protection against atypical observations; and\n",
    "- classical diagnostic tools may not work as advertised.\n",
    "\n",
    "Our example contains $n = 200$ observations with $p = 6$\n",
    "explanatory variables. The regression model is $Y = \n",
    "V1 + 2*V2 + V3 + V4 + V5 + \\varepsilon$, where \n",
    "$\\varepsilon$ follows a $N(0, 1.7)$ distribution. \n",
    "Hence, the true vector of regression \n",
    "coefficients is `(1, 2, 1, 1, 1, 0)` and the true intercept is \n",
    "zero. The explanatory variables are all independent standard\n",
    "normal random variables. I used the following code \n",
    "to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161523bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 200\n",
    "p <- 6\n",
    "set.seed(123)\n",
    "x0 <- as.data.frame(matrix(rnorm(n*p), n, p))\n",
    "x0$y <- with(x0, V1 + 2*V2 + V3 + V4 + V5 + rnorm(n, sd=1.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc3ea6b",
   "metadata": {},
   "source": [
    "We now replace the last 20 observations\n",
    "with outliers (for a total proportion of atypical observations of 20/200 = 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b81f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps <- .1\n",
    "n1 <- ceiling(n*(1-eps))\n",
    "x0[n1:n, 1:p] <- matrix(rnorm((n-n1+1)*p, mean=+1.85, sd=.8))\n",
    "x0$y[n1:n] <- rnorm(n-n1+1, mean=-7, sd=1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad90545",
   "metadata": {},
   "source": [
    "These atypical observations cannot be seen easily in \n",
    "a pairwise plot, specially if one does not know \n",
    "in advance that they are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae71299",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "pairs(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67d133",
   "metadata": {},
   "source": [
    "Standard diagnostic plots do not flag anything of \n",
    "importance either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 <- lm(y~., data=x0)\n",
    "par(mfrow=c(2,2))\n",
    "plot(m0, which=c(1, 2, 5))\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c647c7",
   "metadata": {},
   "source": [
    "Note that all the Cook distances are below 0.15, \n",
    "for example. \n",
    "However, the estimated regression coefficients are\n",
    "very different from the true ones\n",
    "(1, 2, 1, 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f8aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbind(LS=coef(m0), Truth=c(0,1, 2, 1, 1, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e498b5a",
   "metadata": {},
   "source": [
    "We now compare the estimated regression \n",
    "coefficients obtained with 2 other methods:\n",
    "an MM-estimator,\n",
    "and the L1-estimator (which is a quantile\n",
    "regression estimator). We will later see in\n",
    "the course that these estimators \n",
    "have different robustness properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033189cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library(RobStatTM)\n",
    "# myc <- lmrobdet.control(family='bisquare', efficiency=.95)\n",
    "# m1 <- lmrobdetMM(y~., data=x0, control=myc)\n",
    "myc <- lmrob.control(family='bisquare', efficiency=.95)\n",
    "m1 <- lmrob(y~., data=x0, control=myc)\n",
    "m3 <- quantreg::rq(y~., data=x0)\n",
    "cbind(Truth=c(0, 1, 2, 1, 1, 1, 0),\n",
    "      LS=coef(m0), L1=coef(m3), MM=coef(m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a1624",
   "metadata": {},
   "source": [
    "We can also compare the diagnostic plots obtained with\n",
    "the robust estimator, where the outliers are\n",
    "clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bbcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "plot(m1, which=c(1, 2, 4))\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c914e42",
   "metadata": {},
   "source": [
    "Add now the LS estimator computed on the \"clean\" data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ecfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0.cl <- lm(y~., data=x0, subset = -(n1:n) )\n",
    "cbind(Truth=c(0, 1, 2, 1, 1, 1, 0),\n",
    "      LS=coef(m0), L1=coef(m3),  \n",
    "      MM=coef(m1), LSclean = coef(m0.cl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3f98d",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "In this section we discuss briefly a class of robust estimators \n",
    "for linear regression models (re-descending M-estimators). This\n",
    "class is known to have good robustness properties \n",
    "(e.g. high breakdown-point) and can be tuned to be \n",
    "highly-efficient when the errors follow a specific distribution. \n",
    "Moreover, the score / loss function can be chosen to improve their\n",
    "asymptotic bias. The main difficulty one encounters with these\n",
    "estimators is computational, since they require to find the \n",
    "minimum of a non-convex function in several dimensions. Much effort\n",
    "has been put in developing good algorithms, and two alternatives\n",
    "will be mentioned below. \n",
    "\n",
    "## M-estimators\n",
    "\n",
    "M-estimators for linear regression are a natural extension \n",
    "of the M-estimators used in the location/scale models. \n",
    "They can be motivated intuitively in a similar manner \n",
    "to that used for the location / scale model&mdash;start \n",
    "with a Gaussian MLE estimator\n",
    "and truncate the loss / score function. Such a monotone score function\n",
    "(corresponding to a convex loss function, but one that grows \n",
    "at a slower rate than the squared loss) was first proposed by \n",
    "Huber (in the location model: [1964](https://doi.org/10.1214/aoms/1177703732), \n",
    "in a more general univariate setting [1967](https://projecteuclid.org/ euclid.bsmsp/1200512988), \n",
    "and for linear regression [1973](https://doi.org/10.1214/aos/1176342503)). \n",
    "Note that the family of M-estimators based on \n",
    "monotone (non-decreasing) score functions includes the L1 (quantile\n",
    "regression) estimators. \n",
    "\n",
    "<!-- The corresponding regression  -->\n",
    "<!-- estimators may have  -->\n",
    "<!-- a very low breakdown point (as low as $$1/p$$, where $$p$$ -->\n",
    "<!-- is the number of features) if high-leverage outliers  -->\n",
    "<!-- (outliers among the explanatory variables) can be  -->\n",
    "<!-- present (see, e.g. [Maronna et al, 1979](https://doi.org/10.1007/BFb0098492)).  -->\n",
    "\n",
    "#### Fixed designs\n",
    "\n",
    "When the explanatory variables are \"fixed\" (in the sense \n",
    "of being \"controlled\", as in a designed experiment, or \n",
    "because they are bounded, for example) then \n",
    "M-estimators with a monotone and bounded score function\n",
    "have high-breakdown point.\n",
    "For example, in this situation\n",
    "quantile regression\n",
    "(L1) estimators are robust (but not efficient). \n",
    "M-estimators computed with a Huber score function\n",
    "will also have high-breakdown point and \n",
    "the score function can be tuned to result in\n",
    "estimators that are also efficient. \n",
    "However, to compute these M-estimators\n",
    "estimators we need to use standardized \n",
    "residuals using an auxiliary\n",
    "(preliminary) residual scale estimator. \n",
    "Similar to what we did in the location model \n",
    "(where we used a robust scale estimator of \n",
    "the residuals computed from the median), \n",
    "here we can use a robust scale estimator of\n",
    "the residuals with respect to the L1 regression\n",
    "estimator, which does not require standardized\n",
    "residuals to be computed. \n",
    "Hence, an effective strategy to obtain high-breakdown point and\n",
    "high-efficiency estimators in this case is as follows:\n",
    "\n",
    "1. Compute the L1 regression estimator;\n",
    "2. Compute `s_n`, an M-estimator of the scale of the corresponding residuals;\n",
    "3. Use `s_n` to compute an M-estimator of regression (e.g. using Huber's loss function). \n",
    "\n",
    "Note that since monotone score functions correspond to \n",
    "convex loss functions, the third step in the algorithm above\n",
    "is computationally relatively simple. \n",
    "\n",
    "It is easy to see that estimators based on monotone score\n",
    "functions may have larger biases than those based on \n",
    "re-descending ones (re-descending score functions are \n",
    "described below). So, a simple variation of the approach above\n",
    "that generally performs better is to use a bounded loss\n",
    "function for the last step above. Since this now implies \n",
    "optimizing a non-convex function, the computational complexity\n",
    "can be prohibitive. However, extensive numerical experiments\n",
    "showed that finding a \"local minimum\" starting from a \"good\"\n",
    "initial point yields an estimator with very good properties,\n",
    "and one that it is very simple to compute. The \n",
    "corresponding algorithm is:\n",
    "\n",
    "1. Compute the L1 regression estimator;\n",
    "2. Compute `s_n`, an M-estimator of the scale of the corresponding residuals;\n",
    "3. Use `s_n` and the L1 regression estimator to start the minimizing iterations \n",
    "of an M-estimator with a re-descending score function. \n",
    "\n",
    "This estimator is implemented in the function `lmrobM` of package `RobStatTM`.\n",
    "As an example, consider the `phosphor` data in package `robustbase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(phosphor, package='robustbase')\n",
    "library(RobStatTM)\n",
    "myc <- lmrobdet.control(family='bisquare', efficiency=.95)\n",
    "ph.M <- lmrobM(plant ~ inorg, data=phosphor, control=myc)\n",
    "plot(plant ~ inorg, data=phosphor, pch=19, cex=1.2)\n",
    "abline(ph.M, lwd=4, col='tomato3')\n",
    "ph.rq <- quantreg::rq(plant ~ inorg, data=phosphor)\n",
    "abline(ph.rq, lwd=4, col='steelblue')\n",
    "legend(5, 160, legend=c('lmrobM fit', 'L1'), lwd=4, col=c('tomato3', 'steelblue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6cacd7",
   "metadata": {},
   "source": [
    "#### Outliers in the explanatory variables\n",
    "\n",
    "If outliers may be present in the explanatory variables \n",
    "(which precludes \"designed\" or \"controlled experiments\" \n",
    "situations), then monotone-M estimators (i.e. M-estimators with \n",
    "a monotone (non-decreasing) $\\psi$ function, such as Huber's \n",
    "M-estimators, or L1 (quantile) regression estimators) may \n",
    "not be robust. \n",
    "\n",
    "As an example, consider the `alcohol` data set (available \n",
    "from package `RobStaTM`). Although several explanatory variables\n",
    "are available, to fix ideas here we focus on a simpler linear\n",
    "regression model with a single covariate `SAG`. \n",
    "\n",
    "We first compare the L1 (quantile regression) estimator and\n",
    "the M-estimator (initialized with the L1 estimator, as described\n",
    "above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(alcohol)\n",
    "a <- quantreg::rq(logSolubility ~ SAG, data=alcohol)\n",
    "b <- lmrobM(logSolubility ~ SAG, data=alcohol)\n",
    "plot(logSolubility ~ SAG, data=alcohol)\n",
    "abline(a, lwd=4, col='gray70')\n",
    "abline(b, col='tomato3', lwd=4)\n",
    "legend(450, -2, legend=c('lmrobM fit', 'L1'), lwd=4, col=c('tomato3', 'gray70'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e1266",
   "metadata": {},
   "source": [
    "Note that both estimators are very close to each other, and capture\n",
    "the linear relationship between the variables. To illustrate the point\n",
    "we are trying to make here, we will move the 3 right-most observations \n",
    "in the data further to the right, and also shift them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "alcohol$logSolubility[c(38, 43:44)] <- \n",
    "  alcohol$logSolubility[c(38, 43:44)] + 15 # /100 + 15, 20 30# 50 15\n",
    "alcohol$SAG[c(38, 43:44)] <- \n",
    "  alcohol$SAG[c(38, 43:44)] + 300 #300  200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c7db5",
   "metadata": {},
   "source": [
    "We now compute again the L1 and monotone M estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "a <- quantreg::rq(logSolubility ~ SAG, data=alcohol)\n",
    "b <- lmrobM(logSolubility ~ SAG, data=alcohol)\n",
    "plot(logSolubility ~ SAG, data=alcohol)\n",
    "abline(a, col='gray70', lwd=4)\n",
    "abline(b, col='tomato3', lwd=4)\n",
    "legend(400, 4, legend=c('lmrobM fit', 'L1'), lwd=4, col=c('tomato3', 'gray70'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630a9a8",
   "metadata": {},
   "source": [
    "Note that neither of these estimators is now able to identify the \n",
    "(linear) relationship that holds for the majority of the data: \n",
    "`r (nrow(alcohol) - 3)`/`r nrow(alcohol)` = `r round((nrow(alcohol) -\n",
    "3)/nrow(alcohol)*100, 2)`%. \n",
    "The proportion of \"bad\" data is only `r 3`/`r nrow(alcohol)`, or \n",
    "`r round(3/nrow(alcohol)*100, 2)`%.\n",
    "\n",
    "However, a properly initialized redescending estimator has no \n",
    "problem at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d <- lmrobdetMM(logSolubility ~ SAG, data=alcohol)\n",
    "plot(logSolubility ~ SAG, data=alcohol)\n",
    "abline(a, col='gray70', lwd=4)\n",
    "abline(b, col='tomato3', lwd=4)\n",
    "abline(d, col='steelblue', lwd=4)\n",
    "legend(400, 4, legend=c('lmrobM fit', 'L1', 'Redesc'), lwd=4, lty=1, \n",
    "       col=c('tomato3', 'gray70', 'steelblue' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb78de",
   "metadata": {},
   "source": [
    "Note that this last estimator also works very well when there \n",
    "are no atypical observations in the data. \n",
    "\n",
    "#### Random features (explanatory variables)\n",
    "\n",
    "When explanatory variables are observed (are part of the\n",
    "random phenomenon being measured), outliers and other\n",
    "atypical data points can be present. Observations that \n",
    "are outlying in the space of features are \n",
    "usually called high-leverage. When such points are \n",
    "present M-estimators for linear regression computed with a monotone \n",
    "score function (e.g. Huber's, or L1 [quantile regression]) \n",
    "may have a breakdown point as low as $$1/p$$, where $$p$$\n",
    "is the number of features (for an illustration, see the first \n",
    "example in the [Lecture 1](Lecture1.md) notes). \n",
    "Some references include\n",
    "[Maronna et al (1979)](https://doi.org/10.1007/BFb0098492) and \n",
    "[Maronna & Yohai (1991)](https://doi.org/10.2307/2290400).\n",
    "\n",
    "A solution to this problem is to use a \n",
    "re-descending score function&mdash;a \n",
    "score function $$\\psi(t)$$ that is zero for \n",
    "$$|t| > c$$ for some $$c > 0$$. This corresponds to \n",
    "a bounded loss function\n",
    "$$\\rho$$. Since bounded loss functions are necessarily non-convex,\n",
    "the optimization problem that defines these estimators \n",
    "is computationally challenging. In particular, there may be \n",
    "several critical points (first-order conditions equal to zero) \n",
    "that do not correspond to the \n",
    "global minimum. However, \n",
    "[Yohai (1987)](https://doi.org/10.1214/aos/1176350366) showed \n",
    "that it is enough to find a local minimum \n",
    "starting from a consistent estimator. This is discussed below\n",
    "in Sections \"S-estimators\" and \"M-estimators with a preliminary scale\".  \n",
    "\n",
    "## The issue of scale\n",
    "\n",
    "An often overlooked problem is that in order to use these estimators\n",
    "in practice we need to estimate the scale (standard deviation, if\n",
    "second moments exist) of the residuals (standardized residuals \n",
    "have to be used in the estimating equations). Naturally, this issue also \n",
    "afects M-estimators for location / scale, but for them it can \n",
    "be solved relatively easily by using the MAD of the observations, \n",
    "for example. Note that this robust residual scale estimator \n",
    "can be computed independently from the M-estimator. In regression models, \n",
    "however, where outliers may be present in the explanatory variables, \n",
    "there is no simple robust regression estimator that \n",
    "can be used to obtain reliable residuals, in order to compute\n",
    "a preliminary residual scale estimator. In other words, to\n",
    "compute a robust regression estimator we need a robust residual\n",
    "scale estimator. But to compute \n",
    "a robust residual\n",
    "scale estimator we need a robust regression estimator (in order\n",
    "to obtain reliable residuals). S-estimators (below) can \n",
    "break this impasse. \n",
    "\n",
    "## S-estimators\n",
    "\n",
    "S-estimators are defined as the regression coefficients \n",
    "that result in residuals that minimize a (robust) \n",
    "of scale. In particular, we use M-estimators of scale, because\n",
    "they are relatively easier to minimize than\n",
    "would be the case if used others (e.g. MAD). \n",
    "These regression estimators can be tuned to have high-breakdown\n",
    "point, but their efficiency is typically low. This is not\n",
    "a concern, as the resulting residual scale estimator is used\n",
    "to compute an M-estimator of regression that can be tuned to have\n",
    "high-efficiency. \n",
    "\n",
    "#### Computational challenges\n",
    "\n",
    "S-estimators can be difficult to compute. They are defined as\n",
    "the point at which a (typically) non-convex function attains its\n",
    "minimum. The loss function that needs to be minimized is only\n",
    "defined implicitly (as the solution to a non-linear equation). \n",
    "However, its gradient can be computed explicitly, and iterative\n",
    "algorithms that decrease the objective function at each step\n",
    "exist ([SB and Yohai (2006)](http://dx.doi.org/10.1198/106186006X113629)).\n",
    "The main computational bottleneck is the need for a \"good\" \n",
    "starting point. Data-dependent random starts have been used \n",
    "for a long time. This approach is implemented in the function \n",
    "`lmrob` of the package `robustbase`. \n",
    "\n",
    "<!-- Here is a simple example, using the well-known  -->\n",
    "<!-- stack loss data (see `help(stackloss)` for more information -->\n",
    "<!-- on these data).  -->\n",
    "<!-- Note that the main objective of `lmrob()` is to compute the -->\n",
    "<!-- subsequent M-estimator,  -->\n",
    "<!-- the S-estimator is included in one entry (`$init.S`) of  -->\n",
    "<!-- the list returned by `lmrob()`.  -->\n",
    "<!-- ```{r stackloss} -->\n",
    "<!-- data(stackloss) -->\n",
    "<!-- set.seed(123) -->\n",
    "<!-- a <- lmrob(stack.loss ~ ., data=stackloss) -->\n",
    "<!-- Sest <- a$init.S -->\n",
    "<!-- coef(Sest) -->\n",
    "<!-- ``` -->\n",
    "<!-- We can look at the fitted vs. residuals plot, and easily -->\n",
    "<!-- identify 4 potential outliers.  -->\n",
    "<!-- ```{r stackloss2}  -->\n",
    "<!-- plot(fitted(Sest), resid(Sest), pch=19, cex=1.1,  -->\n",
    "<!--      xlab='Fitted values', ylab='Residuals') -->\n",
    "<!-- abline(h=Sest$scale*2.5*c(-1, 0, 1), lty=2) -->\n",
    "<!-- n <- length(resid(Sest)) -->\n",
    "<!-- labels.id <- paste(1L:n) -->\n",
    "<!-- iid <- 1:4 -->\n",
    "<!-- show.r <- sort.list(abs(resid(Sest)), decreasing = TRUE)[iid] -->\n",
    "<!-- text(fitted(Sest)[show.r]-1.5, resid(Sest)[show.r],  -->\n",
    "<!--      show.r, cex = 1.1, xpd = TRUE, offset = 0.25) -->\n",
    "<!-- ``` -->\n",
    "\n",
    "## M-estimators with a preliminary scale\n",
    "\n",
    "The function `lmrob` in package `robustbase` implements\n",
    "M-estimators with a re-descending score (bounded loss) function,\n",
    "computed using a preliminary residual scale estimator \n",
    "(an S-estimator as above). This implementation uses data-dependent\n",
    "random starts for the S-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f379af",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "a <- lmrob(stack.loss ~ ., data=stackloss)\n",
    "par(mfrow=c(2,2))\n",
    "plot(a, which=c(1, 2, 4))\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5504d",
   "metadata": {},
   "source": [
    "<!-- Note that the M-estimator identifies fewer outliers than -->\n",
    "<!-- the S-estimator. This is because, by default, the  -->\n",
    "<!-- M-estimator is tuned to have high-efficiency (95% if the -->\n",
    "<!-- errors have a Gaussian distribution), and this induces -->\n",
    "<!-- a relatively high asymptotic bias. If we reduce the -->\n",
    "<!-- efficiency to 85%, then the M-estimator resembles -->\n",
    "<!-- the S- one. We use the function `RobStatTM::bisquare()` -->\n",
    "<!-- to compute the tuning constant the corresponds to  -->\n",
    "<!-- a desired efficiency, for regression estimators -->\n",
    "<!-- computed using Tukey's bisquare loss function.  -->\n",
    "<!-- ```{r stackloss4} -->\n",
    "<!-- library(robustbase) -->\n",
    "<!-- set.seed(123) -->\n",
    "<!-- myc <- lmrob.control(tuning.psi=RobStatTM::bisquare(.85)) -->\n",
    "<!-- a <- lmrob(stack.loss ~ ., data=stackloss, control=myc) -->\n",
    "<!-- par(mfrow=c(2,2)) -->\n",
    "<!-- plot(a, which=c(1, 2, 4), id.n=4) -->\n",
    "<!-- par(mfrow=c(1,1)) -->\n",
    "<!-- ``` -->\n",
    "\n",
    "<!-- The function `lmrobdetMM` in package `RobStatTM` implements -->\n",
    "<!-- a different starting point for the iterative  -->\n",
    "<!-- algorithm that computes the S-estimator. Instead of using -->\n",
    "<!-- data-dependent random starts, a few deterministic starting -->\n",
    "<!-- points are considered.  -->\n",
    "<!-- The code below compares the resulting fit on the `stackloss` data: -->\n",
    "<!-- ```{r stackloss5} -->\n",
    "<!-- library(RobStatTM) -->\n",
    "<!-- set.seed(123) -->\n",
    "<!-- myc <- lmrobdet.control(family='bisquare', efficiency=.95) -->\n",
    "<!-- a.det <- lmrobdetMM(stack.loss ~ ., data=stackloss, control=myc) -->\n",
    "<!-- par(mfrow=c(2,2)) -->\n",
    "<!-- plot(a.det, which=c(1, 2, 4), id.n=4) -->\n",
    "<!-- par(mfrow=c(1,1)) -->\n",
    "<!-- ``` -->\n",
    "\n",
    "<!-- We see that in this case, both estimators yield essentially -->\n",
    "<!-- the same fit -->\n",
    "<!-- ```{r stackcomp} -->\n",
    "<!-- cbind(lmrob=coef(a), lmrobdetMM=coef(a.det)) -->\n",
    "<!-- ``` -->\n",
    "\n",
    "The least squares fit only identifies a single\n",
    "potential mild outlier (observation 21), and the\n",
    "regression coefficients are somewhat different from\n",
    "the robust ones (specially for `Water.Temp` and `Acid.Conc.`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.ls <- lm(stack.loss ~ ., data=stackloss)\n",
    "par(mfrow=c(2,2))\n",
    "plot(a.ls, which=c(1, 2, 5))\n",
    "par(mfrow=c(1,1))\n",
    "cbind(ls=coef(a.ls), lmrob=coef(a)) #, lmrobdetMM=coef(a.det))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70226f0",
   "metadata": {},
   "source": [
    "#### Choosing the score / loss function\n",
    "\n",
    "For this class of M-estimators we can choose the \n",
    "family of loss/score functions, and the corresponding tuning\n",
    "constant. For example, Tukey's bisquare loss is\n",
    "$$\\rho(t) = \\min(k^2/6, k^2/6*(1-(1-(t/k)^2)^3))$$. The next \n",
    "figures illustrate $$\\rho$$ and its derivative $$\\psi$$ (the corresponding\n",
    "score function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt <- seq(-6, 6, length=200)\n",
    "tun.cnst <- bisquare(0.95)\n",
    "par(mfrow=c(2,1))\n",
    "plot(tt, rho(tt, family='bisquare', cc=tun.cnst), type='l', \n",
    "     lwd=4, col='red', xlab='t', ylab=expression(rho(t)))\n",
    "abline(v=0, lty=2)\n",
    "plot(tt, rhoprime(tt, family='bisquare', cc=tun.cnst), type='l', \n",
    "     lwd=4, col='red', xlab='t', ylab=expression(psi(t)))\n",
    "abline(v=0, lty=2); abline(h=0, lty=2); par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b95642",
   "metadata": {},
   "source": [
    "The tuning constant is typically chosen to obtain an estimator\n",
    "with a desired efficiency when the errors follow a specific distribution.\n",
    "For example, the function `bisquare()` used above returns the \n",
    "value of the tuning parameter that should be used with Tukey's \n",
    "family of loss functions to obtain\n",
    "a desired efficiency when errors are Gaussian. \n",
    "Although the breakdown point of these estimators is high (as \n",
    "high as that of the auxiliary S-estimator for the residual scale, \n",
    "which can be chosen to be 50%), and their efficiency can \n",
    "then subsequently be set by selecting an appropriate tuning parameter,\n",
    "there is a bias / variance trade-off (the higher the efficiency (the\n",
    "lower the variance), the higher the asymptotic bias). \n",
    "\n",
    "There is, however, another  \"parameter\" that can be chosen\n",
    "to reduce the bias of the estimator\n",
    "(for a given breakdown point and efficiency)&mdash;the **family \n",
    "of loss functions** itself. The package `RobStatTM` implements the optimal\n",
    "loss function (`opt`), which can be set using the control \n",
    "argument in `lmrobdetMM` (see\n",
    "Section 5.8.1 in [Maronna et al (2019)](https://doi.org/10.1002/9781119214656)).\n",
    "Below we revisit the stack loss example, using a 95% efficient\n",
    "estimator computed with the bias-optimal loss, and\n",
    "compare it with the 95% efficient one based on the bisquare\n",
    "loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c20e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(RobStatTM)\n",
    "set.seed(123)\n",
    "myc <- lmrobdet.control(family='opt', efficiency=.95)\n",
    "a.opt <- lmrobdetMM(stack.loss ~ ., data=stackloss, control=myc)\n",
    "par(mfrow=c(2,2))\n",
    "plot(a.opt, which=c(1, 2, 4), id.n=4)\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede6460",
   "metadata": {},
   "source": [
    "<!-- Note that by using a loss function with better asymptotic bias -->\n",
    "<!-- properties we are able to detect all four outliers detected by -->\n",
    "<!-- the S-estimator but using a highly efficient and robust  -->\n",
    "<!-- regression estimator, which results in better  -->\n",
    "<!-- (e.g. more powerful) inference for the regression parameters.  -->\n",
    "<!-- In other words, we obtain a more efficient regression estimator -->\n",
    "<!-- incurring in a much smaller increase in asymptotic bias, which -->\n",
    "<!-- results in better outlier-detection capabilities. -->\n",
    "\n",
    "The estimated regression parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbind(ls=coef(a.ls), Tukey=coef(a), Opt=coef(a.opt))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
