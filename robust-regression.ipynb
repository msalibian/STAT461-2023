{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4241f388",
   "metadata": {},
   "source": [
    "#### LICENSE\n",
    "These notes are released under the \n",
    "\"Creative Commons Attribution-ShareAlike 4.0 International\" license. \n",
    "See the **human-readable version** [here](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "and the **real thing** [here](https://creativecommons.org/licenses/by-sa/4.0/legalcode). \n",
    "\n",
    "# Linear Regression \n",
    "\n",
    "In this section we briefly introduce robust M-estimators \n",
    "for linear regression models. We will start with \n",
    "so-called \"monotone\" M-estimators, which generalize\n",
    "what we have done in the location/scale model. This class\n",
    "of estimators includes the L1 (quantile regression) \n",
    "estimators. Unfortunately, if outliers are present among\n",
    "the explanatory variables, these M-estimators may not\n",
    "be robust. Furthermore, determining whether outliers are \n",
    "present in the data is often very difficult to do,\n",
    "without using a robust estimator in the first place. \n",
    "We will illustrate these issues with a simple example\n",
    "below. \n",
    "\n",
    "In general, robust M-estimators will need to be \n",
    "based on minimizing a bounded\n",
    "(and thus non-convex) loss function, which \n",
    "creates obvious computational challenges, but also \n",
    "conceptual ones (computing a robust residual scale\n",
    "estimator is not easy). \n",
    "\n",
    "## M-estimators\n",
    "\n",
    "M-estimators for linear regression are a natural extension \n",
    "of the M-estimators used in the location/scale models. \n",
    "They can be motivated intuitively in a similar manner \n",
    "to that used for the location / scale model: start \n",
    "with a Gaussian MLE estimator\n",
    "and truncate the loss / score function. Such a monotone score function\n",
    "(corresponding to a convex loss function that grows \n",
    "at a slower rate than the squared loss) was first proposed by \n",
    "Huber (in the location model: [1964](https://doi.org/10.1214/aoms/1177703732), \n",
    "in a more general univariate setting [1967](https://projecteuclid.org/ euclid.bsmsp/1200512988), \n",
    "and for linear regression [1973](https://doi.org/10.1214/aos/1176342503)). \n",
    "Note that the family of M-estimators based on \n",
    "monotone (non-decreasing) score functions includes the L1 (quantile\n",
    "regression) estimators. \n",
    "\n",
    "### Simple linear regression\n",
    "We will use the `phosphor` data in \n",
    "package `robustbase` (for `R`). \n",
    "Details can be found using `help(phosphor, package='robustbase')`. \n",
    "The response variable is `plant` and, \n",
    "to simplify the example, we will use only one explanatory variable,\n",
    "`organic`. Furthermore, in order to \n",
    "highlight the potential impact of outliers, we will \n",
    "change the position of the single outlier in these data (from the \n",
    "right end of the plot to the left):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(robustbase)\n",
    "data(phosphor)\n",
    "phosphor[17, 'organic'] <- 15\n",
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487fbf2",
   "metadata": {},
   "source": [
    "We now fit the usual least squares estimator \n",
    "plus a robust one  \n",
    "(an MM-estimator, which is a type of M-estimators that \n",
    "will be discussed below) and overlay them on the scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMfit <- lmrob(plant ~ organic, data=phosphor)\n",
    "LSfit <- lm(plant ~ organic, data=phosphor)\n",
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')\n",
    "abline(MMfit, lwd=4, col='hotpink')\n",
    "abline(LSfit, lwd=4, col='steelblue3', lty=1)\n",
    "legend('topright', lwd=3, lty=1, col=c('hotpink', 'steelblue3'), \n",
    "       legend=c('MM', 'LS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1304061",
   "metadata": {},
   "source": [
    "<!-- We look at the estimated regression coefficients: -->\n",
    "<!-- ```{r coef} -->\n",
    "<!-- cbind(MM=coef(MMfit), LS=coef(LSfit)) -->\n",
    "<!-- ``` -->\n",
    "We can easily check that if outliers were not present in the data, then the \n",
    "robust and the least squares estimator coincide. \n",
    "The green line in the next plot corresponds to the OLS estimator\n",
    "computed without the outlier. Note that the robust fit \n",
    "is indistinguishable from the LS one on the clean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plant ~ organic, data=phosphor, pch=19, col='gray50')\n",
    "abline(MMfit, lwd=3, col='hotpink')\n",
    "abline(LSfit, lwd=3, col='steelblue3')\n",
    "LSclean <- lm(plant ~ organic, data=phosphor, subset=-17)\n",
    "abline(LSclean, lwd=3, col='green3')\n",
    "legend('topright', lwd=3, lty=1, \n",
    "       col=c('hotpink', 'steelblue3', 'green3'), \n",
    "       legend=c('MM', 'LS', 'LS(clean)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d8725",
   "metadata": {},
   "source": [
    "Just for completeness, we can also look at the estimated \n",
    "regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ea255",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbind(MM=coef(MMfit), LS=coef(LSfit), LSclean=coef(LSclean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f49b10",
   "metadata": {},
   "source": [
    "### Fixed designs - Monotone M-estimators\n",
    "\n",
    "When the explanatory variables are \"fixed\" (in the sense \n",
    "of being \"controlled\", as in a designed experiment, or \n",
    "because they are intrinsically or naturally bounded, \n",
    "for example) then \n",
    "M-estimators with a monotone and bounded score function\n",
    "(derivative of the loss function)\n",
    "have good robustness properties. \n",
    "For example, in this case, \n",
    "quantile regression\n",
    "(L1) estimators are robust (but not efficient). \n",
    "M-estimators computed with a Huber score function\n",
    "will also have high-breakdown point, and \n",
    "the loss function can be tuned to result in\n",
    "estimators that are also highly efficient.\n",
    "\n",
    "As we discussed in class, to compute these more efficient M-estimators\n",
    "estimators we need a robust \n",
    "residual scale estimator with which to standardize \n",
    "the residuals in the estimating equations.  \n",
    "Similarly to what we did in the location model \n",
    "(where we used a robust scale estimator of \n",
    "the residuals computed from the median), \n",
    "here we can use a robust scale estimator of\n",
    "the residuals with respect to the L1 regression\n",
    "estimator, which does not require standardized\n",
    "residuals to be computed. \n",
    "Hence, an effective strategy to obtain high-breakdown point and\n",
    "high-efficiency estimators in this case is as follows:\n",
    "\n",
    "1. Compute the L1 regression estimator;\n",
    "2. Compute `s_n`, a robust scale estimator of the corresponding residuals;\n",
    "3. Use `s_n` to compute an M-estimator of regression (e.g. using Huber's loss function).\n",
    "\n",
    "Note that since monotone score functions correspond to \n",
    "convex loss functions, the third step in the algorithm above\n",
    "is computationally relatively simple. \n",
    "\n",
    "We will later see that better robustness properties \n",
    "are obtained if the loss function is bounded. In this case\n",
    "the score function (the derivative of the loss) is zero\n",
    "for larger residuals, and thus this type of score\n",
    "functions are called \"re-descending\". \n",
    "\n",
    "So, we will modify the strategy above to compute an M-estimator\n",
    "with a bounded loss in step (3). Since this now implies \n",
    "optimizing a non-convex function, the computational complexity\n",
    "can be prohibitive. However, as we will discuss in class, \n",
    "computing a \"local minimum\" (starting from a \"good\"\n",
    "initial point) yields an estimator with very good \n",
    "robustness and efficiency properties,\n",
    "and one that it is very simple to compute. The \n",
    "corresponding algorithm is:\n",
    "\n",
    "1. Compute the L1 regression estimator;\n",
    "2. Compute `s_n`, a robust scale estimator of the corresponding residuals;\n",
    "3. Use `s_n` and the L1 regression estimator to start the minimizing iterations \n",
    "of an M-estimator with a bounded loss function. \n",
    "\n",
    "This estimator is implemented in the function `lmrobM` of package `RobStatTM`.\n",
    "As an example, consider the `phosphor` data in package `robustbase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8327ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(RobStatTM)\n",
    "data(phosphor)\n",
    "myc <- lmrobM.control(family='bisquare', efficiency=.95)\n",
    "ph.M <- lmrobM(plant ~ inorg, data=phosphor, control=myc)\n",
    "plot(plant ~ inorg, data=phosphor, pch=19, cex=1.2, col='gray50')\n",
    "abline(ph.M, lwd=4, col='tomato3')\n",
    "ph.rq <- quantreg::rq(plant ~ inorg, data=phosphor)\n",
    "abline(ph.rq, lwd=4, col='steelblue')\n",
    "legend('topleft', legend=c('lmrobM fit', 'L1'), \n",
    "       lwd=4, col=c('tomato3', 'steelblue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d57703",
   "metadata": {},
   "source": [
    "### Outliers in the explanatory variables\n",
    "\n",
    "If outliers may be present in the explanatory variables \n",
    "(which precludes \"designed\" or \"controlled experiments\" \n",
    "situations), then monotone-M estimators (i.e. M-estimators with \n",
    "a monotone (non-decreasing) $\\psi$ function, such as Huber's \n",
    "M-estimators, or L1 (quantile) regression estimators) may \n",
    "not be robust. \n",
    "\n",
    "As an example, consider the `alcohol` data set (available \n",
    "from package `RobStaTM`). Although several explanatory variables\n",
    "are available, to fix ideas here we focus on a simpler linear\n",
    "regression model with a single covariate `SAG`. \n",
    "\n",
    "We first compare the L1 (quantile regression) estimator and\n",
    "the M-estimator (initialized with the L1 estimator, as described\n",
    "above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(alcohol)\n",
    "a <- quantreg::rq(logSolubility ~ SAG, data=alcohol)\n",
    "b <- lmrobM(logSolubility ~ SAG, data=alcohol, control=myc)\n",
    "plot(logSolubility ~ SAG, data=alcohol, pch=19, cex=1.2, col='gray50')\n",
    "abline(a, lwd=4, col='steelblue3')\n",
    "abline(b, col='tomato3', lwd=4)\n",
    "legend('topright', legend=c('lmrobM fit', 'L1'), lwd=4, col=c('tomato3', 'steelblue3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca88269",
   "metadata": {},
   "source": [
    "Note that both estimators are very close to each other, and capture\n",
    "the linear relationship between the variables. To illustrate the point\n",
    "we are trying to make here, we will move the 3 right-most observations \n",
    "in the data further to the right, and also shift them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec773ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alcohol$logSolubility[c(38, 43:44)] <- \n",
    "  alcohol$logSolubility[c(38, 43:44)] + 15 \n",
    "alcohol$SAG[c(38, 43:44)] <- \n",
    "  alcohol$SAG[c(38, 43:44)] + 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc251d6b",
   "metadata": {},
   "source": [
    "We now compute again the L1 and monotone M estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0614d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a <- quantreg::rq(logSolubility ~ SAG, data=alcohol)\n",
    "b <- lmrobM(logSolubility ~ SAG, data=alcohol, control=myc)\n",
    "plot(logSolubility ~ SAG, data=alcohol, pch=19, cex=1.2, col='gray50')\n",
    "abline(a, col='steelblue3', lwd=4)\n",
    "abline(b, col='tomato3', lwd=4)\n",
    "legend('topleft', legend=c('lmrobM fit', 'L1'), lwd=4, col=c('tomato3', 'steelblue3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd097c3",
   "metadata": {},
   "source": [
    "Note that neither of these estimators is now able to identify the \n",
    "(linear) relationship that holds for the majority of the data: \n",
    "`r (nrow(alcohol) - 3)`/`r nrow(alcohol)` = `r round((nrow(alcohol) -\n",
    "3)/nrow(alcohol)*100, 2)`%. \n",
    "The proportion of \"bad\" data is only `r 3`/`r nrow(alcohol)`, or \n",
    "`r round(3/nrow(alcohol)*100, 2)`%.\n",
    "\n",
    "However, a properly initialized redescending M-estimator has no \n",
    "problem at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8895274",
   "metadata": {},
   "outputs": [],
   "source": [
    "d <- lmrob(logSolubility ~ SAG, data=alcohol)\n",
    "plot(logSolubility ~ SAG, data=alcohol, pch=19, cex=1.2, col='gray50')\n",
    "abline(a, col='steelblue3', lwd=4)\n",
    "abline(b, col='tomato3', lwd=4)\n",
    "abline(d, col='hotpink', lwd=4)\n",
    "legend('topleft', legend=c('lmrobM fit', 'L1', 'Redesc'), lwd=4, lty=1, \n",
    "       col=c('tomato3', 'steelblue3', 'hotpink' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54217d41",
   "metadata": {},
   "source": [
    "Note that this last estimator also works very well when there \n",
    "are no atypical observations in the data. \n",
    "\n",
    "\n",
    "### A synthetic toy example (diagnostics and estimation)\n",
    "\n",
    "This example will illustrate that:\n",
    "\n",
    "- outliers can be severely damaging without being \"obviously\" apparent;\n",
    "- quantile regression estimators (L1) offer limited protection against atypical observations; and\n",
    "- classical diagnostic tools may not work as advertised.\n",
    "\n",
    "Our example contains $n = 200$ observations with $p = 6$\n",
    "explanatory variables. The regression model is $Y = \n",
    "V1 + 2*V2 + V3 + V4 + V5 + \\varepsilon$, where \n",
    "$\\varepsilon$ follows a $N(0, 1.7)$ distribution. \n",
    "Hence, the true vector of regression \n",
    "coefficients is `(1, 2, 1, 1, 1, 0)` and the true intercept is \n",
    "zero. The explanatory variables are all independent standard\n",
    "normal random variables. I used the following code \n",
    "to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 200\n",
    "p <- 6\n",
    "set.seed(123)\n",
    "x0 <- as.data.frame(matrix(rnorm(n*p), n, p))\n",
    "x0$y <- with(x0, V1 + 2*V2 + V3 + V4 + V5 + rnorm(n, sd=1.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b14a0bf",
   "metadata": {},
   "source": [
    "We now replace the last 20 observations\n",
    "with outliers (for a total proportion of atypical observations of 20/200 = 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps <- .1\n",
    "n1 <- ceiling(n*(1-eps))\n",
    "x0[n1:n, 1:p] <- matrix(rnorm((n-n1+1)*p, mean=+1.85, sd=.8))\n",
    "x0$y[n1:n] <- rnorm(n-n1+1, mean=-7, sd=1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed7726",
   "metadata": {},
   "source": [
    "These atypical observations cannot be seen easily in \n",
    "a pairwise plot, specially if one does not know \n",
    "in advance that they are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a19eb",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "message": "FALSE,",
     "warning": "FALSE"
    }
   },
   "outputs": [],
   "source": [
    "pairs(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229bdbef",
   "metadata": {},
   "source": [
    "Standard diagnostic plots do not flag anything of \n",
    "importance either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a58882",
   "metadata": {
    "attributes": {
     "classes": [],
     "fig.height": "7",
     "fig.width": "7,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "m0 <- lm(y~., data=x0)\n",
    "par(mfrow=c(2,2))\n",
    "plot(m0, which=c(1, 2, 5))\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a8063",
   "metadata": {},
   "source": [
    "Note that all the Cook distances are below 0.15, \n",
    "for example. \n",
    "However, the estimated regression coefficients are\n",
    "very different from the true ones\n",
    "(1, 2, 1, 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce096b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbind(Truth=c(0,1, 2, 1, 1, 1, 0), LS=coef(m0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5120e",
   "metadata": {},
   "source": [
    "We now compare the LS regression \n",
    "estimator and the L1-estimator (which is a quantile\n",
    "regression estimator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c749db",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 <- quantreg::rq(y~., data=x0)\n",
    "cbind(Truth=c(0, 1, 2, 1, 1, 1, 0),\n",
    "      LS=coef(m0), L1=coef(m3)) #, MM=coef(m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba5813",
   "metadata": {},
   "source": [
    "Note that the L1 estimator is similarly affected by these\n",
    "outliers. \n",
    "\n",
    "Not surprisingly, the approach described above (of using \n",
    "the L1 estimator to compute residuals, and use their estimated\n",
    "scale to compute an M-estimator), does not work well either\n",
    "(the result is in fact an estimator very close to LS!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946fd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 <- lmrobM(y ~ ., data=x0, control=myc)\n",
    "cbind(Truth=c(0, 1, 2, 1, 1, 1, 0),\n",
    "      LS=coef(m0), L1=coef(m3), L1M=coef(m2)) #, MM=coef(m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27acb2e0",
   "metadata": {},
   "source": [
    "So, in this case, none of the strategies above \n",
    "provide reliable regression estimators. \n",
    "**And** we cannot even detect that outliers may\n",
    "be present in the data. \n",
    "\n",
    "We will later discuss in class a strategy to compute\n",
    "robust regression estimators that can deal with situations\n",
    "like this. They are called MM estimators, and are\n",
    "implemented in the function `lmrob` of the \n",
    "`robustbase` package. \n",
    "We compute it now and show that it indeed provides\n",
    "a much better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81812644",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 <- lmrob(y~., data=x0, \n",
    "            control=lmrob.control(family='bisquare', efficiency=.95))\n",
    "cbind(Truth=c(0, 1, 2, 1, 1, 1, 0),\n",
    "      LS=coef(m0), L1=coef(m3), L1M=coef(m2), \n",
    "      MM=coef(m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431eed7a",
   "metadata": {},
   "source": [
    "Moreover, the estimator is\n",
    "very close (again) to the LS estimator we would have\n",
    "obtained had we known which points were atypical. \n",
    "In this sense, the robust estimator behaves like an\n",
    "**\"oracle estimator\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0.cl <- lm(y~., data=x0, subset = -(n1:n) )\n",
    "cbind(Truth=c(0, 1, 2, 1, 1, 1, 0),\n",
    "      LS=coef(m0), L1=coef(m3), L1M=coef(m2), \n",
    "      MM=coef(m1), LSclean = coef(m0.cl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6994e6",
   "metadata": {},
   "source": [
    "Finally, we can also look at the diagnostic plots obtained with\n",
    "the robust estimator, where the outliers are now\n",
    "clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5103f",
   "metadata": {
    "attributes": {
     "classes": [],
     "fig.height": "7",
     "fig.width": "7,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "plot(m1, which=c(1, 2, 4))\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9932785",
   "metadata": {},
   "source": [
    "### Random features (explanatory variables)\n",
    "\n",
    "When explanatory variables are observed (are part of the\n",
    "random phenomenon being measured), outliers and other\n",
    "atypical data points can be present. Observations that \n",
    "are outlying in the space of features are \n",
    "usually called high-leverage. When such points are \n",
    "present M-estimators for linear regression computed with a monotone \n",
    "score function (e.g. Huber's, or L1 [quantile regression]) \n",
    "may have a breakdown point as low as $1/p$, where $p$\n",
    "is the number of features (for an illustration, see the first \n",
    "example in the [Lecture 1](Lecture1.md) notes). \n",
    "Some references include\n",
    "[Maronna et al (1979)](https://doi.org/10.1007/BFb0098492) and \n",
    "[Maronna & Yohai (1991)](https://doi.org/10.2307/2290400).\n",
    "\n",
    "A solution to this problem is to use a \n",
    "re-descending score function: a \n",
    "score function $\\psi(t)$ that is zero for \n",
    "$|t| > c$ for some $c > 0$. This corresponds to \n",
    "a bounded loss function\n",
    "$\\rho$. Since bounded loss functions are necessarily non-convex,\n",
    "the optimization problem that defines these estimators \n",
    "is computationally challenging. In particular, there may be \n",
    "several critical points (first-order conditions equal to zero) \n",
    "that do not correspond to the \n",
    "global minimum. However, \n",
    "[Yohai (1987)](https://doi.org/10.1214/aos/1176350366) showed \n",
    "that it is enough to find a local minimum \n",
    "starting from a consistent estimator. This is discussed below\n",
    "in Sections \"S-estimators\" and \"M-estimators with a preliminary scale\".  \n",
    "\n",
    "<!-- The corresponding regression  -->\n",
    "<!-- estimators may have  -->\n",
    "<!-- a very low breakdown point (as low as $$1/p$$, where $$p$$ -->\n",
    "<!-- is the number of features) if high-leverage outliers  -->\n",
    "<!-- (outliers among the explanatory variables) can be  -->\n",
    "<!-- present (see, e.g. [Maronna et al, 1979](https://doi.org/10.1007/BFb0098492)).  -->\n",
    "\n",
    "\n",
    "## The issue of scale\n",
    "\n",
    "An often overlooked problem is that in order to use these estimators\n",
    "in practice we need to estimate the scale (standard deviation, if\n",
    "second moments exist) of the residuals (standardized residuals \n",
    "have to be used in the estimating equations). Naturally, this issue also \n",
    "afects M-estimators for location / scale, but for them it can \n",
    "be solved relatively easily by using the MAD of the observations, \n",
    "for example. Note that this robust residual scale estimator \n",
    "can be computed independently from the M-estimator. In regression models, \n",
    "however, where outliers may be present in the explanatory variables, \n",
    "there is no simple robust regression estimator that \n",
    "can be used to obtain reliable residuals, in order to compute\n",
    "a preliminary residual scale estimator. In other words, to\n",
    "compute a robust regression estimator we need a robust residual\n",
    "scale estimator. But to compute \n",
    "a robust residual\n",
    "scale estimator we need a robust regression estimator (in order\n",
    "to obtain reliable residuals). S-estimators can \n",
    "break this impasse. \n",
    "\n",
    "## S-estimators\n",
    "\n",
    "S-estimators are defined as the regression coefficients \n",
    "that result in residuals that minimize a (robust) \n",
    "estimator\n",
    "of scale. In particular, we use M-estimators of scale, because\n",
    "they are relatively easier to minimize in practice than\n",
    "would be the case if used others like the MAD. \n",
    "These regression estimators can be tuned to have high-breakdown\n",
    "point, but their efficiency is typically low. This is not\n",
    "a concern, as the resulting residual scale estimator is used\n",
    "to compute an M-estimator of regression that can be tuned to have\n",
    "high-efficiency. \n",
    "\n",
    "### Computational challenges\n",
    "\n",
    "S-estimators can be difficult to compute. They are defined as\n",
    "the point at which a (typically) non-convex function attains its\n",
    "minimum. The loss function that needs to be minimized is only\n",
    "defined implicitly (as the solution to a non-linear equation). \n",
    "However, its gradient can be computed explicitly, and iterative\n",
    "algorithms that decrease the objective function at each step\n",
    "exist ([SB and Yohai (2006)](http://dx.doi.org/10.1198/106186006X113629)).\n",
    "The main computational bottleneck is the need for a \"good\" \n",
    "starting point. Data-dependent random starts have been used \n",
    "for a long time. This approach is implemented in the function \n",
    "`lmrob` of the package `robustbase`. \n",
    "\n",
    "<!-- Here is a simple example, using the well-known  -->\n",
    "<!-- stack loss data (see `help(stackloss)` for more information -->\n",
    "<!-- on these data).  -->\n",
    "<!-- Note that the main objective of `lmrob()` is to compute the -->\n",
    "<!-- subsequent M-estimator,  -->\n",
    "<!-- the S-estimator is included in one entry (`$init.S`) of  -->\n",
    "<!-- the list returned by `lmrob()`.  -->\n",
    "<!-- ```{r stackloss} -->\n",
    "<!-- data(stackloss) -->\n",
    "<!-- set.seed(123) -->\n",
    "<!-- a <- lmrob(stack.loss ~ ., data=stackloss) -->\n",
    "<!-- Sest <- a$init.S -->\n",
    "<!-- coef(Sest) -->\n",
    "<!-- ``` -->\n",
    "<!-- We can look at the fitted vs. residuals plot, and easily -->\n",
    "<!-- identify 4 potential outliers.  -->\n",
    "<!-- ```{r stackloss2}  -->\n",
    "<!-- plot(fitted(Sest), resid(Sest), pch=19, cex=1.1,  -->\n",
    "<!--      xlab='Fitted values', ylab='Residuals') -->\n",
    "<!-- abline(h=Sest$scale*2.5*c(-1, 0, 1), lty=2) -->\n",
    "<!-- n <- length(resid(Sest)) -->\n",
    "<!-- labels.id <- paste(1L:n) -->\n",
    "<!-- iid <- 1:4 -->\n",
    "<!-- show.r <- sort.list(abs(resid(Sest)), decreasing = TRUE)[iid] -->\n",
    "<!-- text(fitted(Sest)[show.r]-1.5, resid(Sest)[show.r],  -->\n",
    "<!--      show.r, cex = 1.1, xpd = TRUE, offset = 0.25) -->\n",
    "<!-- ``` -->\n",
    "\n",
    "## M-estimators with a preliminary scale\n",
    "\n",
    "The function `lmrob` in package `robustbase` implements\n",
    "M-estimators with a re-descending score (bounded loss) function,\n",
    "computed using a preliminary residual scale estimator \n",
    "(an S-estimator as above). This implementation uses data-dependent\n",
    "random starts for the S-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8f5ec",
   "metadata": {
    "attributes": {
     "classes": [],
     "fig.height": "7",
     "fig.width": "7,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "a <- lmrob(stack.loss ~ ., data=stackloss)\n",
    "par(mfrow=c(2,2))\n",
    "plot(a, which=c(1, 2, 4))\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1728a2",
   "metadata": {},
   "source": [
    "<!-- Note that the M-estimator identifies fewer outliers than -->\n",
    "<!-- the S-estimator. This is because, by default, the  -->\n",
    "<!-- M-estimator is tuned to have high-efficiency (95% if the -->\n",
    "<!-- errors have a Gaussian distribution), and this induces -->\n",
    "<!-- a relatively high asymptotic bias. If we reduce the -->\n",
    "<!-- efficiency to 85%, then the M-estimator resembles -->\n",
    "<!-- the S- one. We use the function `RobStatTM::bisquare()` -->\n",
    "<!-- to compute the tuning constant the corresponds to  -->\n",
    "<!-- a desired efficiency, for regression estimators -->\n",
    "<!-- computed using Tukey's bisquare loss function.  -->\n",
    "<!-- ```{r stackloss4} -->\n",
    "<!-- library(robustbase) -->\n",
    "<!-- set.seed(123) -->\n",
    "<!-- myc <- lmrob.control(tuning.psi=RobStatTM::bisquare(.85)) -->\n",
    "<!-- a <- lmrob(stack.loss ~ ., data=stackloss, control=myc) -->\n",
    "<!-- par(mfrow=c(2,2)) -->\n",
    "<!-- plot(a, which=c(1, 2, 4), id.n=4) -->\n",
    "<!-- par(mfrow=c(1,1)) -->\n",
    "<!-- ``` -->\n",
    "\n",
    "<!-- The function `lmrobdetMM` in package `RobStatTM` implements -->\n",
    "<!-- a different starting point for the iterative  -->\n",
    "<!-- algorithm that computes the S-estimator. Instead of using -->\n",
    "<!-- data-dependent random starts, a few deterministic starting -->\n",
    "<!-- points are considered.  -->\n",
    "<!-- The code below compares the resulting fit on the `stackloss` data: -->\n",
    "<!-- ```{r stackloss5} -->\n",
    "<!-- library(RobStatTM) -->\n",
    "<!-- set.seed(123) -->\n",
    "<!-- myc <- lmrobdet.control(family='bisquare', efficiency=.95) -->\n",
    "<!-- a.det <- lmrobdetMM(stack.loss ~ ., data=stackloss, control=myc) -->\n",
    "<!-- par(mfrow=c(2,2)) -->\n",
    "<!-- plot(a.det, which=c(1, 2, 4), id.n=4) -->\n",
    "<!-- par(mfrow=c(1,1)) -->\n",
    "<!-- ``` -->\n",
    "\n",
    "<!-- We see that in this case, both estimators yield essentially -->\n",
    "<!-- the same fit -->\n",
    "<!-- ```{r stackcomp} -->\n",
    "<!-- cbind(lmrob=coef(a), lmrobdetMM=coef(a.det)) -->\n",
    "<!-- ``` -->\n",
    "\n",
    "The least squares fit only identifies a single\n",
    "potential mild outlier (observation 21), and the\n",
    "regression coefficients are somewhat different from\n",
    "the robust ones (specially for `Water.Temp` and `Acid.Conc.`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d9b75",
   "metadata": {
    "attributes": {
     "classes": [],
     "fig.height": "7",
     "fig.width": "7,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "a.ls <- lm(stack.loss ~ ., data=stackloss)\n",
    "par(mfrow=c(2,2))\n",
    "plot(a.ls, which=c(1, 2, 5))\n",
    "par(mfrow=c(1,1))\n",
    "cbind(ls=coef(a.ls), lmrob=coef(a)) #, lmrobdetMM=coef(a.det))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a38800",
   "metadata": {},
   "source": [
    "### Choosing the score / loss function\n",
    "\n",
    "For this class of M-estimators we can choose the \n",
    "family of loss/score functions, and the corresponding tuning\n",
    "constant. For example, Tukey's bisquare loss is\n",
    "$\\rho(t) = \\min(k^2/6, k^2/6*(1-(1-(t/k)^2)^3))$. The next \n",
    "figures illustrate $\\rho$ and its derivative $\\psi$ (the corresponding\n",
    "score function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt <- seq(-6, 6, length=200)\n",
    "tun.cnst <- bisquare(0.95)\n",
    "par(mfrow=c(2,1))\n",
    "plot(tt, rho(tt, family='bisquare', cc=tun.cnst), type='l', \n",
    "     lwd=4, col='red', xlab='t', ylab=expression(rho(t)))\n",
    "abline(v=0, lty=2)\n",
    "plot(tt, rhoprime(tt, family='bisquare', cc=tun.cnst), type='l', \n",
    "     lwd=4, col='red', xlab='t', ylab=expression(psi(t)))\n",
    "abline(v=0, lty=2); abline(h=0, lty=2); par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0027f",
   "metadata": {},
   "source": [
    "The tuning constant is typically chosen to obtain an estimator\n",
    "with a desired efficiency when the errors follow a specific distribution.\n",
    "For example, the function `bisquare()` used above returns the \n",
    "value of the tuning parameter that should be used with Tukey's \n",
    "family of loss functions to obtain\n",
    "a desired efficiency when errors are Gaussian. \n",
    "Although the breakdown point of these estimators is high (as \n",
    "high as that of the auxiliary S-estimator for the residual scale, \n",
    "which can be chosen to be 50%), and their efficiency can \n",
    "then subsequently be set by selecting an appropriate tuning parameter,\n",
    "there is a bias / variance trade-off (the higher the efficiency (the\n",
    "lower the variance), the higher the asymptotic bias). \n",
    "\n",
    "There is, however, another  \"parameter\" that can be chosen\n",
    "to reduce the bias of the estimator\n",
    "(for a given breakdown point and efficiency)&mdash;the **family \n",
    "of loss functions** itself. The package `RobStatTM` implements the optimal\n",
    "loss function (`opt`), which can be set using the control \n",
    "argument in `lmrobdetMM` (see\n",
    "Section 5.8.1 in [Maronna et al (2019)](https://doi.org/10.1002/9781119214656)).\n",
    "Below we revisit the stack loss example, using a 95% efficient\n",
    "estimator computed with the bias-optimal loss, and\n",
    "compare it with the 95% efficient one based on the bisquare\n",
    "loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c76e66",
   "metadata": {
    "attributes": {
     "classes": [],
     "fig.height": "7",
     "fig.width": "7,",
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "library(RobStatTM)\n",
    "set.seed(123)\n",
    "myc <- lmrobdet.control(family='opt', efficiency=.95)\n",
    "a.opt <- lmrobdetMM(stack.loss ~ ., data=stackloss, control=myc)\n",
    "par(mfrow=c(2,2))\n",
    "plot(a.opt, which=c(1, 2, 4), id.n=4)\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1321e6a",
   "metadata": {},
   "source": [
    "<!-- Note that by using a loss function with better asymptotic bias -->\n",
    "<!-- properties we are able to detect all four outliers detected by -->\n",
    "<!-- the S-estimator but using a highly efficient and robust  -->\n",
    "<!-- regression estimator, which results in better  -->\n",
    "<!-- (e.g. more powerful) inference for the regression parameters.  -->\n",
    "<!-- In other words, we obtain a more efficient regression estimator -->\n",
    "<!-- incurring in a much smaller increase in asymptotic bias, which -->\n",
    "<!-- results in better outlier-detection capabilities. -->\n",
    "\n",
    "The estimated regression parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ae597",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbind(ls=coef(a.ls), Tukey=coef(a), Opt=coef(a.opt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
